# Model Training and Ensembling Strategy

## Overview

- Multiple multi-task models were trained using **distinct sets of hyperparameters**, based on:
  - **Chemprop**
  - **CheMelenon foundation model**

- Additionally, **four KERMT-based multi-task models** were trained.

- All models were evaluated using **10-fold cross-validation**.

## Task Configuration

- **All tasks** were trained in a **multi-task learning setting**, **except for logD**:
  - The `logD` task used **publicly available data**
  - It was trained as a **single-task model**

## Model Ensembling

- Final predictions were generated by **ensembling only the models** that achieved a **test MAE below a specified cutoff threshold**.